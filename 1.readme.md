# Omniverse In-Memory Simulation Engine (CLI Edition)

## Overview
A compact, monolithic simulation system that enables in-memory, multi-agent environments driven by local LLMs and lightweight RAG DBs. Designed for low-to-mid range systems, the engine dynamically allocates system resources, scales nodes, and uses CLI interaction for theory-based simulation and feedback loops.

### Key Features
- CLI-based simulation control
- Dynamic node spawning based on available RAM and CPU
- In-memory SQLite + FAISS for retrieval-augmented generation (RAG)
- GPU offloading (if supported)
- Supports lightweight LLMs via Ollama (0.5B to 1B param)
- In-memory runtime (no persistent daemons)
- PDF/.txt ingest via `/ref` command
- Graceful shutdown with `/exit`
- Minimal overhead

---

## System Requirements
- **OS:** Windows 11, Linux, or macOS
- **Python:** 3.10+
- **RAM:** Minimum 8 GB (12 GB recommended)
- **Disk Space:** ~5 GB free (for models and dependencies)
- **Optional:** GPU (if supported by LLM backend, CUDA acceleration)
- **Optional:** Docker (for containerized node support)

---

## Dependencies
Install the following Python packages:

```bash
pip install langchain faiss-cpu psutil docker transformers huggingface-hub PyMuPDF rich
```

Note: Use `faiss-gpu` instead of `faiss-cpu` if your system supports CUDA acceleration.

---

## Install & Configure Ollama
1. Visit [Ollama](https://ollama.com) and follow the installation instructions for your OS.
2. Pull and quantize the model (e.g., MPT-1B) with Ollama:

```bash
ollama pull mosaicml/mpt-1b
ollama quantize mosaicml/mpt-1b --q4_k
```

You can substitute this with any compatible small LLM that Ollama supports.

---

## Optional: Docker Support
To enable isolated container nodes:
1. Install Docker Desktop (Windows/macOS) or Docker Engine (Linux).
2. Start Docker daemon before launching the simulation using the `--docker` flag.

---

## Running the Simulator
From the project root directory, run the simulation with:

```bash
python 1.py --model mpt-1b-q4k
```

### Optional arguments:
- `--docker`        — Enable Docker containers per node
- `--max-nodes N`    — Limit the number of spawned nodes
- `--node-mem N`     — Memory (in MB) assigned to each node

---

## Using the CLI
While the simulation is running, use the following commands:

- `/ref file.txt`       — Load a .txt file into the simulation memory
- `/ref paper.pdf`      — Load a .pdf document into RAG DB
- `/exit`               — Gracefully exit the simulation loop

Any other text input will be sent to all simulation agents as context.

### Example:
```text
/ref theory.txt
> What's the next hypothesis for intelligent simulation layering?
```

---

## File Structure

```
1.py                  # Main simulation orchestrator
rag_omniverse.db      # SQLite memory store
rag_index.faiss       # FAISS index file
1.readme.md           # Project overview
```

---

## Tips for Optimization
- Use quantized LLMs (q4_k or q8_0) for best performance on 8–12 GB RAM systems
- Avoid Docker mode on low RAM devices (<10 GB)
- Lower `AGENT_CYCLES` to reduce run time and CPU usage
- Run with minimal background processes to optimize performance
